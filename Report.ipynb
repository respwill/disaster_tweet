{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a67b41",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "**Objective:** classify tweet if it is regarding disaster or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca6b3d",
   "metadata": {},
   "source": [
    "# Plan to sovle the problem\n",
    "**Used Package**:\n",
    "* Pytorch-lightning\n",
    "* Hugging-Face transformer model\n",
    "\n",
    "**Model architecture:**  \n",
    "pre-trained model  \n",
    "* distilbert-base-uncased  \n",
    "* twitter-xlm-roberta-base-sentiment\n",
    "\n",
    "**Preprocessing:**\n",
    "* Lower case\n",
    "* Contraction fix\n",
    "* Remove urls\n",
    "* Remove emails\n",
    "* Remove HTML\n",
    "* Remove mention tag (@)\n",
    "* Replace imogi\n",
    "* Remove accent\n",
    "* remove unicode\n",
    "* remove punctuation\n",
    "* remove stopwords\n",
    "* remove extra spaces\n",
    "* lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89eac1",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e99ad6",
   "metadata": {},
   "source": [
    "Fine tuned 2 pretrained language models that are from huggingface.  \n",
    "Distilber model shows slightly better performance.  \n",
    "As adjust threshold, the best F1 score is 0.84  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9c49e",
   "metadata": {},
   "source": [
    "|Model|Accuracy|F1 score|Dataset|Threshold|\n",
    "|----|---------|--------|-------|---------|\n",
    "|distilbert-base-uncased|0.86|0.83|Validation|0.5|\n",
    "|distilbert-base-uncased|0.86|0.84|Validation|0.4363273|\n",
    "|cardiffnlp/twitter-xlm-roberta-base-sentiment|0.85|0.81|Validation|0.5|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e48de",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9de8c",
   "metadata": {},
   "source": [
    "For each model, I tested 3 different learning rate: 0.0003, 0.00003, 0.000003   \n",
    "Test1 is result of 0.003, Test2 is result of 0.00003, and so on.  \n",
    "\n",
    "Test1 and Test2 case shows overfitting within 10 epochs and validation loss smoothly decreased in Test3 case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6cf833",
   "metadata": {},
   "source": [
    "![alt text](Validation_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad3c9d",
   "metadata": {},
   "source": [
    "![alt text](Validation_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b509c",
   "metadata": {},
   "source": [
    "![alt text](train_loss.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
